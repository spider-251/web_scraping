{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = \"http://192.198.126.179:7222\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_link = []\n",
    "Category = []\n",
    "def reed_extractor():#this functions extracts the job links from the particular websie\n",
    "    try:\n",
    "        for i in range(0,10): # iterates upto 10 pages\n",
    "            url_next='https://www.reed.co.uk/jobs/data-science-jobs?pageno='+str(i)\n",
    "            w1=requests.get(url_next,proxy) # to access the url\n",
    "            soups=BeautifulSoup(w1.content,'html.parser') \n",
    "            soup = soups.findAll(class_=\"job-block-link gtmJobTitleClickResponsive\", href=True) #find all the data from html of respective class\n",
    "            for item in soup: # iterates through the items which has href links\n",
    "                if item['href'].startswith('/jobs/'):\n",
    "                    job_link.append('https://www.reed.co.uk'+item['href']) # adds the iterted data to the empty list\n",
    "                    Category.append('Data Science')\n",
    "    except:\n",
    "        pass         \n",
    "    return job_link,Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_link,Category=reed_extractor() # calls the funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_posting_df=pd.DataFrame({\"Category\":Category,\"Job_Posting_link\":job_link})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_posting_df.to_csv('Data_science_reed_links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"Data_science_reed_links.csv\")\n",
    "ml = pd.read_csv('Machine_Learning_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reed_details_extractor(df): # this function extracts the data from the particular job link\n",
    "    reed_df = pd.DataFrame()\n",
    "    for i in range(0,len(df)): #it iterates through the links upto the extracted links length\n",
    "        try:\n",
    "            Job_Posting_link=df['Job_Posting_link'][i] #iterates through the links in the dataframe\n",
    "            Category=df['Category'][i]\n",
    "            w2=requests.get(Job_Posting_link,proxy)\n",
    "            soups1=BeautifulSoup(w2.content,'html.parser') \n",
    "            try:\n",
    "                Location=soups1.find(class_='location col-xs-12 col-sm-6 col-md-6 col-lg-6').text #extracts the text data from the respective class\n",
    "                Location = \" \".join(Location.split()) #clears the data by deleting unwanted spaces and new line charachters\n",
    "                pass\n",
    "            except:\n",
    "                Location\n",
    "            try:\n",
    "                salary = soups1.find(class_= 'salary col-xs-12 col-sm-6 col-md-6 col-lg-6').text\n",
    "                salary = \" \".join(salary.split())\n",
    "                pass\n",
    "            except:\n",
    "                salary = None\n",
    "            try:\n",
    "                company = soups1.find(class_ = 'posted').find('a').text\n",
    "                company = \" \".join(company.split())\n",
    "                pass\n",
    "            except:\n",
    "                company=None\n",
    "            try:\n",
    "                title = soups1.find(class_ = 'job-header row').find(\"h1\").text\n",
    "                pass\n",
    "            except:\n",
    "                title=None\n",
    "            try:\n",
    "                desc = soups1.find(class_='description').find('span').text\n",
    "                pass\n",
    "            except:\n",
    "                desc=None\n",
    "            reed=pd.DataFrame({'Category':Category,'Job_Posting_link':Job_Posting_link,'Job_title':title,'Company_name':company,'Location':Location,'Description':[desc]},index=[0])\n",
    "            reed_df=reed_df.append(reed)\n",
    "        except Exception as e: # raises an exception if something goes wrong in the time of extraction\n",
    "            print(e)   \n",
    "    return reed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "data_science = reed_details_extractor(ds) #calls the above funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_learning = reed_details_extractor(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science.to_csv(\"reed_Machine_Learning_full_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_learning.to_csv(\"reed_data_science_full_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9958a2ee1ad7ce5bed12c6dea2733b89eb37390d2e7528d87543b903c6138a8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
